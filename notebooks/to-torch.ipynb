{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import entropy, ks_2samp, moment, wasserstein_distance, energy_distance\n",
    "\n",
    "\n",
    "# Computing metrics for different archtypes\n",
    "def compute_divergences(A, B):\n",
    "    \"\"\" Compute divergence metrics (Jensen Shannon, Kullback-Liebler,\n",
    "    Wasserstein Distance, Energy Distance) between predicted distribution A\n",
    "    and true distribution B \"\"\"\n",
    "\n",
    "    # Get number of samples, IQR statistics, range\n",
    "    samples = A.shape[0]\n",
    "    iqr = np.percentile(A, 75)-np.percentile(A, 25)\n",
    "    r = np.max(A) - np.min(A)\n",
    "\n",
    "    # Get PDFs of predicted distribution A, true distribution B\n",
    "    B = get_pdf(B, iqr, r, samples)\n",
    "    A = get_pdf(A, iqr, r, samples)\n",
    "    \n",
    "    # Mean\n",
    "    m = (np.array(A)+np.array(B))/2\n",
    "\n",
    "    # Compute metrics\n",
    "    kl = entropy(pk=A, qk=B).sum()/A.shape[1]\n",
    "    js = .5*(entropy(pk=A, qk=m)+entropy(pk=B, qk=m)).sum()/A.shape[1]\n",
    "    wd = sum([wasserstein_distance(A[:,i], B[:,i]) for i in range(A.shape[1])])\n",
    "    ed = sum([energy_distance(A[:,i], B[:,i]) for i in range(A.shape[1])])\n",
    "\n",
    "    divergences = {\"KL-Divergence\": kl,\n",
    "                    \"Jensen-Shannon\": js,\n",
    "                    \"Wasserstein-Distance\": wd,\n",
    "                    \"Energy-Distance\": ed}\n",
    "\n",
    "    return divergences\n",
    "\n",
    "\n",
    "def get_pdf(data, iqr, r, samples):\n",
    "    \"\"\" Compute optimally binned probability distribution function  \"\"\"\n",
    "    x = []\n",
    "\n",
    "    if iqr > 1e-5:\n",
    "        bin_width = 2*iqr/np.cbrt(samples)\n",
    "        bins = int(round(r/bin_width, 0))\n",
    "    else:\n",
    "        # MNIST (since it's really only supposed to be either 0 or 1 as output)\n",
    "        # TODO: bin number\n",
    "        bins = 2\n",
    "\n",
    "    # Bin data\n",
    "    for i in range(data.shape[1]):\n",
    "        x.append(list(np.histogram(data[:, i], bins=bins, density=True)[0]))\n",
    "    \n",
    "    res = np.array(x).T\n",
    "    res[res == 0] = .00001\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_info(tensor, q1, q2):\n",
    "    reshaped = tensor.view(-1)\n",
    "    vals, _ = torch.sort(reshaped)\n",
    "    lower_index = torch.tensor(len(vals)*(q1/100.), dtype=torch.long)\n",
    "    upper_index = torch.tensor(len(vals)*(q2/100.), dtype=torch.long)\n",
    "    iqr, r = vals[upper_index]-vals[lower_index], max(reshaped)-min(reshaped)\n",
    "    return iqr, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(100, 384)\n",
    "B = torch.rand(100, 384)\n",
    "\n",
    "# Get IQR, range\n",
    "iqr, r = pdf_info(A, 25, 75)\n",
    "\n",
    "# Get number of samples, IQR statistics, range\n",
    "samples = A.shape[0]\n",
    "\n",
    "# Compute optimal number of bins\n",
    "if iqr > 1e-5:\n",
    "    bin_width = 2*iqr/np.cbrt(samples)\n",
    "    bins = int(torch.round(r/bin_width))\n",
    "else:\n",
    "    # MNIST (since it's really only supposed to be either 0 or 1 as output)\n",
    "    bins = 2\n",
    "\n",
    "\n",
    "# Bin data\n",
    "for i in range(A.shape[1]):\n",
    "    split_sizes = list(torch.histc(A[:, i].unsqueeze(0), bins=bins))\n",
    "    binned = torch.split(A[:, i], split_sizes)\n",
    "    \n",
    "# x = torch.stack(x, dim=0).t()\n",
    "# x[x == 0.] = .0001\n",
    "# res = np.array(x).T\n",
    "# res[res == 0] = .00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96379181, 1.16669534, 1.318873  , 0.6594365 , 0.96379181])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(A[:, i], bins=bins, density=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5120, 0.8679, 0.5436, 0.5864, 0.5975, 0.7310, 0.5374, 0.9440, 0.0122,\n",
       "        0.5917, 0.7504, 0.3882, 0.5232, 0.7612, 0.9199, 0.3626, 0.1818, 0.1558,\n",
       "        0.2462])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.2129, 11.5778, 13.4894,  5.9807,  7.4806])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([i.sum() for i in torch.split(A[:, i], split_sizes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5120, 0.8679, 0.5436, 0.5864, 0.5975, 0.7310, 0.5374, 0.9440, 0.0122,\n",
       "         0.5917, 0.7504, 0.3882, 0.5232, 0.7612, 0.9199, 0.3626, 0.1818, 0.1558,\n",
       "         0.2462]),\n",
       " tensor([0.0470, 0.6908, 0.2849, 0.0107, 0.5306, 0.3337, 0.1801, 0.5450, 0.8865,\n",
       "         0.3464, 0.3411, 0.1018, 0.7874, 0.9964, 0.5675, 0.6523, 0.6392, 0.8868,\n",
       "         0.2185, 0.4355, 0.9826, 0.8220, 0.2912]),\n",
       " tensor([0.3101, 0.0808, 0.4335, 0.1517, 0.8076, 0.1835, 0.7917, 0.7890, 0.9808,\n",
       "         0.6403, 0.7438, 0.8658, 0.2525, 0.8125, 0.6645, 0.5700, 0.1434, 0.3205,\n",
       "         0.5207, 0.8382, 0.3805, 0.3723, 0.4196, 0.4059, 0.5293, 0.4812]),\n",
       " tensor([0.5569, 0.4898, 0.8138, 0.9287, 0.3409, 0.9035, 0.5990, 0.1810, 0.2557,\n",
       "         0.3364, 0.2599, 0.1176, 0.1975]),\n",
       " tensor([0.4249, 0.9302, 0.2209, 0.1877, 0.3374, 0.0939, 0.0505, 0.8372, 0.0981,\n",
       "         0.2128, 0.2179, 0.7116, 0.4839, 0.0936, 0.2991, 0.8020, 0.5533, 0.4802,\n",
       "         0.4455]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.16046955, 1.21092474, 0.90819356, 0.75682796, 1.00910395])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(A[:, i], bins=bins, density=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.histc(, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = sorted(A[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19819563627243042"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max(sort) - min(sort)) / bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 8, 11,  8,  9, 12,  7, 16, 11,  9,  9]),\n",
       " array([0.00870156, 0.10467611, 0.20065066, 0.2966252 , 0.39259975,\n",
       "        0.4885743 , 0.58454884, 0.68052339, 0.77649794, 0.87247248,\n",
       "        0.96844703]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(A[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8353577"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:, 0][8:19].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6e8997320ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-6e8997320ef2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "[sum(np.histogram(A[:, i])[0])[0:i+1] for idx in range(len(np.histogram(A[:, i])[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7077a7bdcdee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "np.histogram(A[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-da84260c6cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "for i in np.histogram(A[:, i]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Distribution()"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.distribution.Distribution(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function kl_div in module torch.nn.functional:\n",
      "\n",
      "kl_div(input, target, size_average=None, reduce=None, reduction='elementwise_mean')\n",
      "    The `Kullback-Leibler divergence`_ Loss.\n",
      "    \n",
      "    See :class:`~torch.nn.KLDivLoss` for details.\n",
      "    \n",
      "    Args:\n",
      "        input: Tensor of arbitrary shape\n",
      "        target: Tensor of the same shape as input\n",
      "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "            the losses are averaged over each loss element in the batch. Note that for\n",
      "            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "            when reduce is ``False``. Default: ``True``\n",
      "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "            losses are averaged or summed over observations for each minibatch depending\n",
      "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "            'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
      "            'elementwise_mean': the sum of the output will be divided by the number of\n",
      "            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
      "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "            specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F.kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(F.log_softmax(A, dim=0), F.softmax(B, dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
