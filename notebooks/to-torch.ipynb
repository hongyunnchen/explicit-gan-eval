{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(100, 384)\n",
    "B = torch.rand(100, 384)\n",
    "# A = A.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_info(tensor, q1, q2):\n",
    "    reshaped = tensor.view(-1)\n",
    "    vals, _ = torch.sort(reshaped)\n",
    "    lower_index = torch.tensor(len(vals)*(q1/100.), dtype=torch.long)\n",
    "    upper_index = torch.tensor(len(vals)*(q2/100.), dtype=torch.long)\n",
    "    iqr, r = vals[upper_index]-vals[lower_index], max(reshaped)-min(reshaped)\n",
    "    return iqr, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr, r = pdf_info(A, 25, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iqr > 1e-5:\n",
    "    bin_width = 2*iqr/np.cbrt(samples)\n",
    "    bins = int(torch.round(r/bin_width))\n",
    "else:\n",
    "    # MNIST (since it's really only supposed to be either 0 or 1 as output)\n",
    "    # TODO: bin number\n",
    "    bins = 2\n",
    "\n",
    "# Bin data\n",
    "x = []\n",
    "for i in range(A.shape[1]):\n",
    "    split_sizes = list(torch.histc(A[:, i].unsqueeze(0), bins=bins))\n",
    "#     x.append(torch.histc(A[:, i].unsqueeze(0), bins=bins))\n",
    "    \n",
    "# x = torch.stack(x, dim=0).t()\n",
    "# x[x == 0.] = .0001\n",
    "# res = np.array(x).T\n",
    "# res[res == 0] = .00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Distribution()"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.distributions.distribution.Distribution(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function kl_div in module torch.nn.functional:\n",
      "\n",
      "kl_div(input, target, size_average=None, reduce=None, reduction='elementwise_mean')\n",
      "    The `Kullback-Leibler divergence`_ Loss.\n",
      "    \n",
      "    See :class:`~torch.nn.KLDivLoss` for details.\n",
      "    \n",
      "    Args:\n",
      "        input: Tensor of arbitrary shape\n",
      "        target: Tensor of the same shape as input\n",
      "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "            the losses are averaged over each loss element in the batch. Note that for\n",
      "            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "            when reduce is ``False``. Default: ``True``\n",
      "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "            losses are averaged or summed over observations for each minibatch depending\n",
      "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "            'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
      "            'elementwise_mean': the sum of the output will be divided by the number of\n",
      "            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
      "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "            specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F.kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(F.log_softmax(A, dim=0), F.softmax(B, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy, ks_2samp, moment, wasserstein_distance, energy_distance\n",
    "\n",
    "# Computing metrics for different archtypes\n",
    "def compute_divergences(A, B):\n",
    "    \"\"\" Compute divergence metrics (Jensen Shannon, Kullback-Liebler,\n",
    "    Wasserstein Distance, Energy Distance) between predicted distribution A\n",
    "    and true distribution B \"\"\"\n",
    "\n",
    "    # Get number of samples, IQR statistics, range\n",
    "    samples = A.shape[0]\n",
    "    iqr = np.percentile(A, 75)-np.percentile(A, 25)\n",
    "    r = np.max(A) - np.min(A)\n",
    "\n",
    "    # Get PDFs of predicted distribution A, true distribution B\n",
    "    B = get_pdf(B, iqr, r, samples)\n",
    "    A = get_pdf(A, iqr, r, samples)\n",
    "    \n",
    "    # Mean\n",
    "    m = (np.array(A)+np.array(B))/2\n",
    "\n",
    "    # Compute metrics\n",
    "    kl = entropy(pk=A, qk=B).sum()/A.shape[1]\n",
    "    js = .5*(entropy(pk=A, qk=m)+entropy(pk=B, qk=m)).sum()/A.shape[1]\n",
    "    wd = sum([wasserstein_distance(A[:,i], B[:,i]) for i in range(A.shape[1])])\n",
    "    ed = sum([energy_distance(A[:,i], B[:,i]) for i in range(A.shape[1])])\n",
    "\n",
    "    divergences = {\"KL-Divergence\": kl,\n",
    "                    \"Jensen-Shannon\": js,\n",
    "                    \"Wasserstein-Distance\": wd,\n",
    "                    \"Energy-Distance\": ed}\n",
    "\n",
    "    return divergences\n",
    "\n",
    "\n",
    "def get_pdf(data, iqr, r, samples):\n",
    "    \"\"\" Compute optimally binned probability distribution function  \"\"\"\n",
    "    x = []\n",
    "\n",
    "    if iqr > 1e-5:\n",
    "        bin_width = 2*iqr/np.cbrt(samples)\n",
    "        bins = int(round(r/bin_width, 0))\n",
    "    else:\n",
    "        # MNIST (since it's really only supposed to be either 0 or 1 as output)\n",
    "        # TODO: bin number\n",
    "        bins = 2\n",
    "\n",
    "    # Bin data\n",
    "    for i in range(data.shape[1]):\n",
    "        x.append(list(np.histogram(data[:, i], bins=bins, density=True)[0]))\n",
    "    \n",
    "    res = np.array(x).T\n",
    "    res[res == 0] = .00001\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 212 ms, sys: 6.86 ms, total: 219 ms\n",
      "Wall time: 244 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "A = torch.rand(1024, 512)\n",
    "B = torch.rand(1024, 512)\n",
    "divs = compute_divergences(A.numpy(), B.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tensors = torch.sort(A, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15083., 15536., 15636., 15445., 15468., 15251., 15510., 15575., 15628.,\n",
       "        15440., 15289., 15433., 15451., 15381., 15356., 15403., 15544., 15322.,\n",
       "        15228., 15354., 15345., 15427., 15361., 15291., 15482., 15476., 15276.,\n",
       "        15434., 15417., 15167., 15367., 15610., 15508., 15794.])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.histc(sorted_tensors, bins=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function diff in module numpy.lib.function_base:\n",
      "\n",
      "diff(a, n=1, axis=-1)\n",
      "    Calculate the n-th discrete difference along the given axis.\n",
      "    \n",
      "    The first difference is given by ``out[n] = a[n+1] - a[n]`` along\n",
      "    the given axis, higher differences are calculated by using `diff`\n",
      "    recursively.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input array\n",
      "    n : int, optional\n",
      "        The number of times values are differenced. If zero, the input\n",
      "        is returned as-is.\n",
      "    axis : int, optional\n",
      "        The axis along which the difference is taken, default is the\n",
      "        last axis.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    diff : ndarray\n",
      "        The n-th differences. The shape of the output is the same as `a`\n",
      "        except along `axis` where the dimension is smaller by `n`. The\n",
      "        type of the output is the same as the type of the difference\n",
      "        between any two elements of `a`. This is the same as the type of\n",
      "        `a` in most cases. A notable exception is `datetime64`, which\n",
      "        results in a `timedelta64` output array.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    gradient, ediff1d, cumsum\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Type is preserved for boolean arrays, so the result will contain\n",
      "    `False` when consecutive elements are the same and `True` when they\n",
      "    differ.\n",
      "    \n",
      "    For unsigned integer arrays, the results will also be unsigned. This\n",
      "    should not be surprising, as the result is consistent with\n",
      "    calculating the difference directly:\n",
      "    \n",
      "    >>> u8_arr = np.array([1, 0], dtype=np.uint8)\n",
      "    >>> np.diff(u8_arr)\n",
      "    array([255], dtype=uint8)\n",
      "    >>> u8_arr[1,...] - u8_arr[0,...]\n",
      "    array(255, np.uint8)\n",
      "    \n",
      "    If this is not desirable, then the array should be cast to a larger\n",
      "    integer type first:\n",
      "    \n",
      "    >>> i16_arr = u8_arr.astype(np.int16)\n",
      "    >>> np.diff(i16_arr)\n",
      "    array([-1], dtype=int16)\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> x = np.array([1, 2, 4, 7, 0])\n",
      "    >>> np.diff(x)\n",
      "    array([ 1,  2,  3, -7])\n",
      "    >>> np.diff(x, n=2)\n",
      "    array([  1,   1, -10])\n",
      "    \n",
      "    >>> x = np.array([[1, 3, 6, 10], [0, 5, 6, 8]])\n",
      "    >>> np.diff(x)\n",
      "    array([[2, 3, 4],\n",
      "           [5, 1, 2]])\n",
      "    >>> np.diff(x, axis=0)\n",
      "    array([[-1,  2,  0, -2]])\n",
      "    \n",
      "    >>> x = np.arange('1066-10-13', '1066-10-16', dtype=np.datetime64)\n",
      "    >>> np.diff(x)\n",
      "    array([1, 1], dtype='timedelta64[D]')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = torch.max(torch.sort(A, dim=1)[0], dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0817)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(F.log_softmax(A, dim=1), F.softmax(B, dim=1), reduction='sum') / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 512])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 512])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = compute_divergences(A.numpy(), B.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KL-Divergence': 0.008894706257342695,\n",
       " 'Jensen-Shannon': 0.0022205919229779,\n",
       " 'Wasserstein-Distance': 18.00936629283771,\n",
       " 'Energy-Distance': 51.749713484890535}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.8 ms, sys: 2.59 ms, total: 11.4 ms\n",
      "Wall time: 8.93 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "A = torch.rand(1024, 16)\n",
    "B = torch.rand(1024, 16)\n",
    "A, B  = compute_divergences(A.numpy(), B.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function histogram in module numpy.lib.function_base:\n",
      "\n",
      "histogram(a, bins=10, range=None, normed=False, weights=None, density=None)\n",
      "    Compute the histogram of a set of data.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Input data. The histogram is computed over the flattened array.\n",
      "    bins : int or sequence of scalars or str, optional\n",
      "        If `bins` is an int, it defines the number of equal-width\n",
      "        bins in the given range (10, by default). If `bins` is a\n",
      "        sequence, it defines the bin edges, including the rightmost\n",
      "        edge, allowing for non-uniform bin widths.\n",
      "    \n",
      "        .. versionadded:: 1.11.0\n",
      "    \n",
      "        If `bins` is a string from the list below, `histogram` will use\n",
      "        the method chosen to calculate the optimal bin width and\n",
      "        consequently the number of bins (see `Notes` for more detail on\n",
      "        the estimators) from the data that falls within the requested\n",
      "        range. While the bin width will be optimal for the actual data\n",
      "        in the range, the number of bins will be computed to fill the\n",
      "        entire range, including the empty portions. For visualisation,\n",
      "        using the 'auto' option is suggested. Weighted data is not\n",
      "        supported for automated bin size selection.\n",
      "    \n",
      "        'auto'\n",
      "            Maximum of the 'sturges' and 'fd' estimators. Provides good\n",
      "            all around performance.\n",
      "    \n",
      "        'fd' (Freedman Diaconis Estimator)\n",
      "            Robust (resilient to outliers) estimator that takes into\n",
      "            account data variability and data size.\n",
      "    \n",
      "        'doane'\n",
      "            An improved version of Sturges' estimator that works better\n",
      "            with non-normal datasets.\n",
      "    \n",
      "        'scott'\n",
      "            Less robust estimator that that takes into account data\n",
      "            variability and data size.\n",
      "    \n",
      "        'rice'\n",
      "            Estimator does not take variability into account, only data\n",
      "            size. Commonly overestimates number of bins required.\n",
      "    \n",
      "        'sturges'\n",
      "            R's default method, only accounts for data size. Only\n",
      "            optimal for gaussian data and underestimates number of bins\n",
      "            for large non-gaussian datasets.\n",
      "    \n",
      "        'sqrt'\n",
      "            Square root (of data size) estimator, used by Excel and\n",
      "            other programs for its speed and simplicity.\n",
      "    \n",
      "    range : (float, float), optional\n",
      "        The lower and upper range of the bins.  If not provided, range\n",
      "        is simply ``(a.min(), a.max())``.  Values outside the range are\n",
      "        ignored. The first element of the range must be less than or\n",
      "        equal to the second. `range` affects the automatic bin\n",
      "        computation as well. While bin width is computed to be optimal\n",
      "        based on the actual data within `range`, the bin count will fill\n",
      "        the entire range including portions containing no data.\n",
      "    normed : bool, optional\n",
      "        This keyword is deprecated in NumPy 1.6.0 due to confusing/buggy\n",
      "        behavior. It will be removed in NumPy 2.0.0. Use the ``density``\n",
      "        keyword instead. If ``False``, the result will contain the\n",
      "        number of samples in each bin. If ``True``, the result is the\n",
      "        value of the probability *density* function at the bin,\n",
      "        normalized such that the *integral* over the range is 1. Note\n",
      "        that this latter behavior is known to be buggy with unequal bin\n",
      "        widths; use ``density`` instead.\n",
      "    weights : array_like, optional\n",
      "        An array of weights, of the same shape as `a`.  Each value in\n",
      "        `a` only contributes its associated weight towards the bin count\n",
      "        (instead of 1). If `density` is True, the weights are\n",
      "        normalized, so that the integral of the density over the range\n",
      "        remains 1.\n",
      "    density : bool, optional\n",
      "        If ``False``, the result will contain the number of samples in\n",
      "        each bin. If ``True``, the result is the value of the\n",
      "        probability *density* function at the bin, normalized such that\n",
      "        the *integral* over the range is 1. Note that the sum of the\n",
      "        histogram values will not be equal to 1 unless bins of unity\n",
      "        width are chosen; it is not a probability *mass* function.\n",
      "    \n",
      "        Overrides the ``normed`` keyword if given.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    hist : array\n",
      "        The values of the histogram. See `density` and `weights` for a\n",
      "        description of the possible semantics.\n",
      "    bin_edges : array of dtype float\n",
      "        Return the bin edges ``(length(hist)+1)``.\n",
      "    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    histogramdd, bincount, searchsorted, digitize\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    All but the last (righthand-most) bin is half-open.  In other words,\n",
      "    if `bins` is::\n",
      "    \n",
      "      [1, 2, 3, 4]\n",
      "    \n",
      "    then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n",
      "    the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n",
      "    *includes* 4.\n",
      "    \n",
      "    .. versionadded:: 1.11.0\n",
      "    \n",
      "    The methods to estimate the optimal number of bins are well founded\n",
      "    in literature, and are inspired by the choices R provides for\n",
      "    histogram visualisation. Note that having the number of bins\n",
      "    proportional to :math:`n^{1/3}` is asymptotically optimal, which is\n",
      "    why it appears in most estimators. These are simply plug-in methods\n",
      "    that give good starting points for number of bins. In the equations\n",
      "    below, :math:`h` is the binwidth and :math:`n_h` is the number of\n",
      "    bins. All estimators that compute bin counts are recast to bin width\n",
      "    using the `ptp` of the data. The final bin count is obtained from\n",
      "    ``np.round(np.ceil(range / h))`.\n",
      "    \n",
      "    'Auto' (maximum of the 'Sturges' and 'FD' estimators)\n",
      "        A compromise to get a good value. For small datasets the Sturges\n",
      "        value will usually be chosen, while larger datasets will usually\n",
      "        default to FD.  Avoids the overly conservative behaviour of FD\n",
      "        and Sturges for small and large datasets respectively.\n",
      "        Switchover point is usually :math:`a.size \\approx 1000`.\n",
      "    \n",
      "    'FD' (Freedman Diaconis Estimator)\n",
      "        .. math:: h = 2 \\frac{IQR}{n^{1/3}}\n",
      "    \n",
      "        The binwidth is proportional to the interquartile range (IQR)\n",
      "        and inversely proportional to cube root of a.size. Can be too\n",
      "        conservative for small datasets, but is quite good for large\n",
      "        datasets. The IQR is very robust to outliers.\n",
      "    \n",
      "    'Scott'\n",
      "        .. math:: h = \\sigma \\sqrt[3]{\\frac{24 * \\sqrt{\\pi}}{n}}\n",
      "    \n",
      "        The binwidth is proportional to the standard deviation of the\n",
      "        data and inversely proportional to cube root of ``x.size``. Can\n",
      "        be too conservative for small datasets, but is quite good for\n",
      "        large datasets. The standard deviation is not very robust to\n",
      "        outliers. Values are very similar to the Freedman-Diaconis\n",
      "        estimator in the absence of outliers.\n",
      "    \n",
      "    'Rice'\n",
      "        .. math:: n_h = 2n^{1/3}\n",
      "    \n",
      "        The number of bins is only proportional to cube root of\n",
      "        ``a.size``. It tends to overestimate the number of bins and it\n",
      "        does not take into account data variability.\n",
      "    \n",
      "    'Sturges'\n",
      "        .. math:: n_h = \\log _{2}n+1\n",
      "    \n",
      "        The number of bins is the base 2 log of ``a.size``.  This\n",
      "        estimator assumes normality of data and is too conservative for\n",
      "        larger, non-normal datasets. This is the default method in R's\n",
      "        ``hist`` method.\n",
      "    \n",
      "    'Doane'\n",
      "        .. math:: n_h = 1 + \\log_{2}(n) +\n",
      "                        \\log_{2}(1 + \\frac{|g_1|}{\\sigma_{g_1}})\n",
      "    \n",
      "            g_1 = mean[(\\frac{x - \\mu}{\\sigma})^3]\n",
      "    \n",
      "            \\sigma_{g_1} = \\sqrt{\\frac{6(n - 2)}{(n + 1)(n + 3)}}\n",
      "    \n",
      "        An improved version of Sturges' formula that produces better\n",
      "        estimates for non-normal datasets. This estimator attempts to\n",
      "        account for the skew of the data.\n",
      "    \n",
      "    'Sqrt'\n",
      "        .. math:: n_h = \\sqrt n\n",
      "        The simplest and fastest estimator. Only takes into account the\n",
      "        data size.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.histogram([1, 2, 1], bins=[0, 1, 2, 3])\n",
      "    (array([0, 2, 1]), array([0, 1, 2, 3]))\n",
      "    >>> np.histogram(np.arange(4), bins=np.arange(5), density=True)\n",
      "    (array([ 0.25,  0.25,  0.25,  0.25]), array([0, 1, 2, 3, 4]))\n",
      "    >>> np.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3])\n",
      "    (array([1, 4, 1]), array([0, 1, 2, 3]))\n",
      "    \n",
      "    >>> a = np.arange(5)\n",
      "    >>> hist, bin_edges = np.histogram(a, density=True)\n",
      "    >>> hist\n",
      "    array([ 0.5,  0. ,  0.5,  0. ,  0. ,  0.5,  0. ,  0.5,  0. ,  0.5])\n",
      "    >>> hist.sum()\n",
      "    2.4999999999999996\n",
      "    >>> np.sum(hist * np.diff(bin_edges))\n",
      "    1.0\n",
      "    \n",
      "    .. versionadded:: 1.11.0\n",
      "    \n",
      "    Automated Bin Selection Methods example, using 2 peak random data\n",
      "    with 2000 points:\n",
      "    \n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    >>> rng = np.random.RandomState(10)  # deterministic random data\n",
      "    >>> a = np.hstack((rng.normal(size=1000),\n",
      "    ...                rng.normal(loc=5, scale=2, size=1000)))\n",
      "    >>> plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n",
      "    >>> plt.title(\"Histogram with 'auto' bins\")\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
