{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, itertools, json\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "\n",
    "def crawl_directory(dirname):\n",
    "    \"\"\" Walk a nested directory to get all filename ending in a pattern \"\"\"\n",
    "    for path, subdirs, files in os.walk(dirname):\n",
    "        for name in files:\n",
    "            if not name.endswith('.DS_Store'):\n",
    "                yield os.path.join(path, name)\n",
    "\n",
    "\n",
    "def remove_empty_dirs(path):\n",
    "    for root, dirnames, filenames in os.walk(path, topdown=False):\n",
    "        for dirname in dirnames:\n",
    "            remove_empty_dir(os.path.realpath(os.path.join(root, dirname)))\n",
    "\n",
    "\n",
    "def remove_empty_dir(path):\n",
    "    try:\n",
    "        os.rmdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def nested_pickle_dict():\n",
    "    \"\"\" Picklable defaultdict nested dictionaries \"\"\"\n",
    "    return defaultdict(nested_pickle_dict)\n",
    "\n",
    "\n",
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return (a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]).lower()\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, axis=0, confidence=0.95):\n",
    "    n = data.shape[axis]\n",
    "    mu, std = np.nanmean(data, axis=axis), scipy.stats.sem(data, axis=axis, nan_policy='omit')\n",
    "    h = np.ma.getdata(std) * scipy.stats.t.ppf((1 + confidence) / 2., n-1)       \n",
    "\n",
    "    return mu, h, mu-h, mu+h\n",
    "\n",
    "def load_best(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get best performance according to hyperparameter mean minimum \"\"\"\n",
    "\n",
    "dirname = '/Users/sob/Desktop/gan_results/hypertuning/multivariate/'\n",
    "\n",
    "# Same for all..\n",
    "resultnames = [i for i in os.listdir(dirname) if i != '.DS_Store']\n",
    "tuningnames = [i for i in os.listdir('/Users/sob/Desktop/gan_results/hypertuning/multivariate/16_dims_1000_samples/trial_1') if '1024' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_1000 = load_best('/Users/sob/Desktop/gan_results/best_1024/multivariate/{0}_dims_1000_samples/data.json'.format(d))\n",
    "optimal_10000 = load_best('/Users/sob/Desktop/gan_results/best_1024/multivariate/{0}_dims_10000_samples/data.json'.format(d))\n",
    "optimal_100000 = load_best('/Users/sob/Desktop/gan_results/best_1024/multivariate/{0}_dims_100000_samples/data.json'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Find best results \"\"\"\n",
    "# for name in tqdm_notebook(resultnames):\n",
    "    \n",
    "#     print(name)\n",
    "    \n",
    "#     best_path = '/Users/sob/Desktop/gan_results/best_1024/multivariate/{0}/'.format(t)\n",
    "#     global_optimal = nested_pickle_dict()\n",
    "\n",
    "#     # Initialize best dictionary    \n",
    "#     for t in tqdm_notebook(tuningnames):\n",
    "\n",
    "#         optimal = nested_pickle_dict()\n",
    "#         results = []\n",
    "\n",
    "#         # Load in the results from each trial\n",
    "#         for trial in range(1, 21):\n",
    "#             path = '/Users/sob/Desktop/gan_results/hypertuning/multivariate/{0}/trial_{1}/{2}'.format(name, trial, t)\n",
    "                \n",
    "#             data = []\n",
    "#             with open(path) as f:\n",
    "#                 for line in f:\n",
    "#                     data.append(json.loads(line))\n",
    "\n",
    "#             results.append(data[0])\n",
    "\n",
    "#         # Go through each one and append the results\n",
    "#         for result in results:\n",
    "#             for model, distributions in result.items():\n",
    "#                 for distribution, metrics in distributions.items():\n",
    "#                     for metric, values in metrics.items():\n",
    "#                         if metric in [\"LR\", \"HDIM\", 'GLoss', 'DLoss', \"BSIZE\", \"Energy-Distance\"]:\n",
    "#                             continue\n",
    "#                         else:\n",
    "#                             # If metric is seen for the first time, initialize it\n",
    "#                             if 'values' not in optimal[model][distribution][metric]:\n",
    "#                                 optimal[model][distribution][metric][\"values\"] = []\n",
    "\n",
    "#                             # Otherwise, compare it the presently considered value\n",
    "#                             optimal[model][distribution][metric][\"values\"].append(values)\n",
    "\n",
    "#         for model, distributions in result.items():\n",
    "#             for distribution, metrics in distributions.items():\n",
    "#                 for metric, values in metrics.items():\n",
    "#                     if metric in [\"LR\", \"HDIM\", 'GLoss', 'DLoss', \"BSIZE\", \"Energy-Distance\"]:\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         data_min = np.nanmean(np.nanmin(np.array(optimal[model][distribution][metric][\"values\"]), axis=1))\n",
    "                        \n",
    "#                         # Init global optimal\n",
    "#                         if 'best' not in global_optimal[model][distribution][metric]:\n",
    "#                             global_optimal[model][distribution][metric]['best'] = 1e10     \n",
    "\n",
    "#                         if data_min < global_optimal[model][distribution][metric]['best']:\n",
    "# #                             print(model, distribution, metric, '\\n', global_optimal[model][distribution][metric]['best'], '-->', data_min, '\\n')\n",
    "#                             global_optimal[model][distribution][metric]['best'] = data_min\n",
    "#                             global_optimal[model][distribution][metric]['parameters'] = [metrics[\"LR\"], metrics[\"HDIM\"], metrics[\"BSIZE\"]]\n",
    "#                             global_optimal[model][distribution][metric][\"values\"] = optimal[model][distribution][metric][\"values\"]\n",
    "            \n",
    "#                             mean, h, low, high = mean_confidence_interval(np.array(optimal[model][distribution][metric][\"values\"]), axis=0)\n",
    "                \n",
    "#                             global_optimal[model][distribution][metric]['low'] = list(low)\n",
    "#                             global_optimal[model][distribution][metric]['h'] = list(h)\n",
    "#                             global_optimal[model][distribution][metric]['mean'] = list(mean)\n",
    "#                             global_optimal[model][distribution][metric]['high'] = list(high)\n",
    "\n",
    "# #     if not os.path.exists(best_path):\n",
    "# #         os.makedirs(best_path)\n",
    "    \n",
    "# #     with open(best_path + 'data.json', 'w') as outfile:\n",
    "# #         json.dump(global_optimal, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_optimal = load_best('/Users/sob/Desktop/gan_results/best_1024/multivariate/{0}/data.json'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20618419, 0.19593402, 0.19403938, 0.19630246, 0.17052982,\n",
       "       0.20884451, 0.19485955, 0.21225419, 0.19609026, 0.18250627,\n",
       "       0.20626169, 0.21324213, 0.19362725, 0.18573761, 0.20808232,\n",
       "       0.20553133, 0.18600923, 0.22414468, 0.22308071, 0.20108958])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmin(np.array(global_optimal[model][distribution][metric][\"values\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22269121198918806"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(global_optimal[model][distribution][metric][\"mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['128_dims_100000_samples',\n",
       " '128_dims_10000_samples',\n",
       " '128_dims_1000_samples',\n",
       " '16_dims_100000_samples',\n",
       " '16_dims_10000_samples',\n",
       " '16_dims_1000_samples',\n",
       " '32_dims_100000_samples',\n",
       " '32_dims_10000_samples',\n",
       " '32_dims_1000_samples',\n",
       " '64_dims_100000_samples',\n",
       " '64_dims_10000_samples',\n",
       " '64_dims_1000_samples']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211342dcb3494c72986f19a0606f6341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_dims_100000_samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6f3db724a248268b0b292cbea4fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_dims_10000_samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432415f274d34f7da4f2c9856e670714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-857598a9118d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Find number of other settings within its confidence interval \"\"\"\n",
    "robust = nested_pickle_dict()\n",
    "j = 0\n",
    "\n",
    "for name in tqdm_notebook(resultnames):\n",
    "    \n",
    "    print(name)\n",
    "    global_optimal = load_best('/Users/sob/Desktop/gan_results/best_1024/multivariate/{0}/data.json'.format(name))\n",
    "\n",
    "    # Initialize best dictionary    \n",
    "    for t in tqdm_notebook(tuningnames):\n",
    "\n",
    "        optimal = nested_pickle_dict()\n",
    "        results = []\n",
    "\n",
    "        # Load in the results from each trial\n",
    "        for trial in range(1, 21):\n",
    "            path = '/Users/sob/Desktop/gan_results/hypertuning/multivariate/{0}/trial_{1}/{2}'.format(name, trial, t)\n",
    "                \n",
    "            data = []\n",
    "            with open(path) as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line))\n",
    "\n",
    "            results.append(data[0])\n",
    "\n",
    "        # Go through each one and append the results\n",
    "        for result in results:\n",
    "            for model, distributions in result.items():\n",
    "                for distribution, metrics in distributions.items():\n",
    "                    for metric, values in metrics.items():\n",
    "                        if metric in [\"LR\", \"HDIM\", 'GLoss', 'DLoss', \"BSIZE\", \"Energy-Distance\"]:\n",
    "                            continue\n",
    "                        else:\n",
    "                                                        \n",
    "                            # If metric is seen for the first time, initialize it\n",
    "                            if 'values' not in optimal[model][distribution][metric]:\n",
    "                                optimal[model][distribution][metric][\"values\"] = []\n",
    "\n",
    "                            # Otherwise, compare it the presently considered value\n",
    "                            optimal[model][distribution][metric][\"values\"].append(values)\n",
    "\n",
    "        for model, distributions in result.items():\n",
    "            for distribution, metrics in distributions.items():\n",
    "                for metric, values in metrics.items():\n",
    "                    if metric in [\"LR\", \"HDIM\", 'GLoss', 'DLoss', \"BSIZE\", \"Energy-Distance\"]:\n",
    "                        continue\n",
    "                    else: \n",
    "                        \n",
    "                        if metric not in robust[model][name][distribution]:\n",
    "                            robust[model][name][distribution][metric] = 0\n",
    "                            \n",
    "                        if 'total' not in robust[model]['all']:\n",
    "                            robust[model]['all']['total'] = 0\n",
    "                            \n",
    "                        if metric not in robust[model]['all']:\n",
    "                            robust[model]['all'][metric] = 0\n",
    "                            \n",
    "                        j += 1\n",
    "                        \n",
    "                        _, _, global_low, global_high = mean_confidence_interval(np.array(global_optimal[model][distribution][metric]['mean']))\n",
    "                        data_mean, _, data_low, data_high = mean_confidence_interval(np.nanmin(np.array(optimal[model][distribution][metric][\"values\"]), axis=1))\n",
    "                        \n",
    "                        if global_low <= data_mean <= global_high:\n",
    "                            \n",
    "                            robust[model][name][distribution][metric] += 1\n",
    "                            robust[model]['all']['total'] += 1\n",
    "                            robust[model]['all'][metric] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgan\n",
      "TOTAL: 23\n",
      "Jensen-Shannon 10\n",
      "KL-Divergence 4\n",
      "Wasserstein-Distance 9\n",
      "\n",
      "\n",
      "began\n",
      "TOTAL: 38\n",
      "Jensen-Shannon 10\n",
      "KL-Divergence 3\n",
      "Wasserstein-Distance 25\n",
      "\n",
      "\n",
      "lsgan\n",
      "TOTAL: 31\n",
      "Jensen-Shannon 9\n",
      "KL-Divergence 4\n",
      "Wasserstein-Distance 18\n",
      "\n",
      "\n",
      "dragan\n",
      "TOTAL: 59\n",
      "Jensen-Shannon 17\n",
      "KL-Divergence 11\n",
      "Wasserstein-Distance 31\n",
      "\n",
      "\n",
      "ragan\n",
      "TOTAL: 34\n",
      "Jensen-Shannon 3\n",
      "KL-Divergence 10\n",
      "Wasserstein-Distance 21\n",
      "\n",
      "\n",
      "wgpgan\n",
      "TOTAL: 78\n",
      "Jensen-Shannon 21\n",
      "KL-Divergence 16\n",
      "Wasserstein-Distance 41\n",
      "\n",
      "\n",
      "fgan_forward_kl\n",
      "TOTAL: 21\n",
      "Jensen-Shannon 2\n",
      "KL-Divergence 4\n",
      "Wasserstein-Distance 15\n",
      "\n",
      "\n",
      "nsgan\n",
      "TOTAL: 13\n",
      "Jensen-Shannon 3\n",
      "KL-Divergence 5\n",
      "Wasserstein-Distance 5\n",
      "\n",
      "\n",
      "infogan\n",
      "TOTAL: 46\n",
      "Jensen-Shannon 13\n",
      "KL-Divergence 4\n",
      "Wasserstein-Distance 29\n",
      "\n",
      "\n",
      "fishergan\n",
      "TOTAL: 23\n",
      "Jensen-Shannon 1\n",
      "KL-Divergence 6\n",
      "Wasserstein-Distance 16\n",
      "\n",
      "\n",
      "fgan_hellinger\n",
      "TOTAL: 23\n",
      "Jensen-Shannon 5\n",
      "KL-Divergence 2\n",
      "Wasserstein-Distance 16\n",
      "\n",
      "\n",
      "fgan_reverse_kl\n",
      "TOTAL: 6\n",
      "Jensen-Shannon 0\n",
      "KL-Divergence 2\n",
      "Wasserstein-Distance 4\n",
      "\n",
      "\n",
      "fgan_total_var\n",
      "TOTAL: 24\n",
      "Jensen-Shannon 4\n",
      "KL-Divergence 5\n",
      "Wasserstein-Distance 15\n",
      "\n",
      "\n",
      "fgan_pearson\n",
      "TOTAL: 28\n",
      "Jensen-Shannon 8\n",
      "KL-Divergence 3\n",
      "Wasserstein-Distance 17\n",
      "\n",
      "\n",
      "fgan_jensen_shannon\n",
      "TOTAL: 18\n",
      "Jensen-Shannon 2\n",
      "KL-Divergence 1\n",
      "Wasserstein-Distance 15\n",
      "\n",
      "\n",
      "mmgan\n",
      "TOTAL: 69\n",
      "Jensen-Shannon 9\n",
      "KL-Divergence 2\n",
      "Wasserstein-Distance 58\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in robust.keys():\n",
    "    print(i)\n",
    "    print('TOTAL:', robust[i]['all']['total'])\n",
    "    for k in ['Jensen-Shannon', 'KL-Divergence', 'Wasserstein-Distance']:\n",
    "        if k == 'total':\n",
    "            continue\n",
    "        print(k, robust[i]['all'][k])\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4320"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_optimal['wgan']['beta']['Jensen-Shannon']['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     try:\n",
    "axis = 0\n",
    "n = data.shape[axis]\n",
    "#     except IndexError:\n",
    "#         axis = 0\n",
    "#         n = data.shape[axis]\n",
    "\n",
    "mu, std = np.nanmean(data, axis=axis), scipy.stats.sem(data, axis=axis, nan_policy='omit')\n",
    "h = np.ma.getdata(std) * scipy.stats.t.ppf((1 + 0.95) / 2., n-1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 25)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1930042 , 0.2066939 , 0.19153797, 0.19525308, 0.20647096,\n",
       "       0.19276595, 0.20471301, 0.19250052, 0.19584609, 0.17902637,\n",
       "       0.20165009, 0.19579905, 0.17772294, 0.21042508, 0.22041611,\n",
       "       0.17814663, 0.18736884, 0.19345119, 0.17377413, 0.18209787])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1939332 , 0.21364977, 0.20867878, 0.20379185, 0.20591492,\n",
       "       0.21686991, 0.20523758, 0.20114066, 0.20765131, 0.20959509,\n",
       "       0.20937313, 0.20580058, 0.20463819, 0.20934051, 0.20512742,\n",
       "       0.20654916, 0.20383895, 0.20391237, 0.20536536, 0.20604479,\n",
       "       0.20699752, 0.20883711, 0.20583202, 0.20704382, 0.20659206])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_optimal[model][distribution]['KL-Divergence'][\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_path = '../best/' + '/'.join(dirname.split('/')[-3:])\n",
    "if not os.path.exists(best_path):\n",
    "    os.makedirs(best_path)\n",
    "\n",
    "files = os.listdir(dirname)\n",
    "files = [f for f in files if f != '.DS_Store']\n",
    "for idx, f in tqdm.tqdm_notebook(enumerate(files)):\n",
    "\n",
    "    optimal = get_best_performance_mnist(dirname + f + '/')\n",
    "    if len(os.listdir(dirname + f + '/')) < 60:\n",
    "        print(f, len(os.listdir(dirname + f + '/')))\n",
    "\n",
    "    with open(best_path + '/trial_{0}.json'.format(idx+1), 'w') as outfile:\n",
    "        json.dump(optimal, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_optimums[model][distribution]['Wasserstein-Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_performance_multivariate(mypath):\n",
    "    \"\"\" For a trial, get the best performance for multivariate data according to any hyperparam \"\"\"\n",
    "    # Get path, files in path\n",
    "    files = os.listdir(mypath)\n",
    "    results = []\n",
    "\n",
    "    # Read in the files\n",
    "    for file in files:\n",
    "        if file == '.DS_Store':\n",
    "            continue\n",
    "                            \n",
    "        with open(mypath + file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        results.append(data)\n",
    "        \n",
    "    # Initialize best dictionary\n",
    "    optimal = nested_pickle_dict()\n",
    "\n",
    "    # Go through all models, distributionss, metrics, and record the best\n",
    "    for result in results:\n",
    "        for model, distributions in result.items():\n",
    "            for distribution, metrics in distributions.items():\n",
    "                for metric, values in metrics.items():\n",
    "                    if metric not in [\"LR\", \"HDIM\", \"BSIZE\"]:\n",
    "\n",
    "                        # If metric is seen for the first time, it is the best\n",
    "                        if metric not in optimal[model][distribution]:\n",
    "                            optimal[model][distribution][metric][\"value\"] = values\n",
    "                            optimal[model][distribution][metric][\"parameters\"] = [metrics[\"LR\"], metrics[\"HDIM\"], metrics[\"BSIZE\"]]\n",
    "\n",
    "                        # Otherwise, compare it the presently considered value\n",
    "                        elif min(optimal[model][distribution][metric][\"value\"]) > min(values):\n",
    "                            optimal[model][distribution][metric][\"value\"] = values\n",
    "                            optimal[model][distribution][metric][\"parameters\"] = [metrics[\"LR\"], metrics[\"HDIM\"], metrics[\"BSIZE\"]]\n",
    "\n",
    "    return optimal\n",
    "\n",
    "\n",
    "def get_best_performance_mnist(*args):\n",
    "    return get_best_performance_multivariate(*args)\n",
    "\n",
    "\n",
    "def multivariate_hypertuning2best(dirname='/Users/sob/Desktop/gan_results/hypertuning/multivariate/64_dims_100000_samples/'):\n",
    "    \"\"\" MOVE HYPERTUNING RESULTS TO BEST FOLDER \"\"\"\n",
    "    best_path = '../best/' + '/'.join(dirname.split('/')[-3:])\n",
    "    if not os.path.exists(best_path):\n",
    "        os.makedirs(best_path)\n",
    "\n",
    "    files = os.listdir(dirname)\n",
    "    files = [f for f in files if f != '.DS_Store']\n",
    "    for idx, f in tqdm.tqdm_notebook(enumerate(files)):\n",
    "\n",
    "        optimal = get_best_performance_mnist(dirname + f + '/')\n",
    "        if len(os.listdir(dirname + f + '/')) < 60:\n",
    "            print(f, len(os.listdir(dirname + f + '/')))\n",
    "\n",
    "        with open(best_path + '/trial_{0}.json'.format(idx+1), 'w') as outfile:\n",
    "            json.dump(optimal, outfile)\n",
    "\n",
    "\n",
    "def merge_mixture(dirname):\n",
    "    outdir = dirname\n",
    "    for idx, file in enumerate(os.listdir(dirname)):\n",
    "\n",
    "        if '.DS_Store' in file:\n",
    "            continue\n",
    "\n",
    "        for nest in crawl_directory(dirname + file):\n",
    "\n",
    "            index = 1\n",
    "\n",
    "            if 'dims' not in nest.split('/')[7]:\n",
    "                outdir = '/'.join(nest.split('/')[:7] + nest.split('/')[8:9]) + '/'\n",
    "            else:\n",
    "                outdir = dirname\n",
    "                \n",
    "            # Initialize directory\n",
    "            if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "            try:\n",
    "                shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "            except:\n",
    "                extension = nest.split('/')[-1]\n",
    "                while os.path.exists(outdir + 'trial_{0}/'.format(index) + extension):\n",
    "                    index += 1\n",
    "\n",
    "                if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                    os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "                shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "    remove_empty_dirs(dirname)\n",
    "    \n",
    "    \n",
    "def merge_multivariate(dirname):\n",
    "    outdir = dirname\n",
    "    for idx, file in enumerate(os.listdir(dirname)):\n",
    "\n",
    "        if '.DS_Store' in file:\n",
    "            continue\n",
    "\n",
    "        for nest in crawl_directory(dirname + file):\n",
    "\n",
    "            index = 1\n",
    "\n",
    "            if 'dims' not in nest.split('/')[6]:\n",
    "                outdir = '/'.join(nest.split('/')[:7] + nest.split('/')[8:9]) + '/'\n",
    "            else:\n",
    "                # Uncomment the + for mixture\n",
    "                outdir = dirname + nest.split('/')[6] + '/'\n",
    "                \n",
    "            # Initialize directory\n",
    "            if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "            try:\n",
    "                shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "            except:\n",
    "                extension = nest.split('/')[-1]\n",
    "                while os.path.exists(outdir + 'trial_{0}/'.format(index) + extension):\n",
    "                    index += 1\n",
    "\n",
    "                if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                    os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "                shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "    remove_empty_dirs(dirname)\n",
    "    \n",
    "    \n",
    "def merge_mnist(dirname):\n",
    "    outdir = dirname\n",
    "    for idx, file in enumerate(os.listdir(dirname)):\n",
    "\n",
    "        if '.DS_Store' in file:\n",
    "            continue\n",
    "\n",
    "        for nest in crawl_directory(dirname + file):\n",
    "\n",
    "            index = 1\n",
    "\n",
    "            if 'dims' in nest.split('/')[5]:\n",
    "                outdir = '/'.join(nest.split('/')[:6]) + '/'\n",
    "            else:\n",
    "                # Uncomment the + for mixture\n",
    "                outdir = dirname + nest.split('/')[7] + '/'\n",
    "    \n",
    "\n",
    "            # Initialize directory\n",
    "            if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "            try:\n",
    "                shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "            except:\n",
    "                extension = nest.split('/')[-1]\n",
    "                while os.path.exists(outdir + 'trial_{0}/'.format(index) + extension):\n",
    "                    index += 1\n",
    "\n",
    "                if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                    os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "                shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "    remove_empty_dirs(dirname)\n",
    "    \n",
    "\n",
    "def get_stats(dirname):\n",
    "    \"\"\" Get missing runs for all trials \"\"\"\n",
    "    hidden_dims = [32, 64, 128, 256, 512]\n",
    "    batch_sizes = [128, 256, 512, 1024]\n",
    "    learning_rates = [2e-1, 2e-2, 2e-3]\n",
    "\n",
    "    filenames, hyperparams = [], []\n",
    "\n",
    "    for (lr, hdim, bsize) in itertools.product(*[learning_rates, hidden_dims, batch_sizes]):\n",
    "        hyperparam = (lr * min(batch_sizes)/bsize, hdim, bsize)\n",
    "        filename = 'results_{0}.json'.format(\"_\".join([str(i) for i in hyperparam]))\n",
    "        filenames.append(filename)\n",
    "        hyperparams.append((str(format_e(lr)), str(hdim), str(bsize)))\n",
    "    \n",
    "    TODO = []\n",
    "    for file in os.listdir(dirname):\n",
    "        if '.DS_Store' in file:\n",
    "            continue\n",
    "\n",
    "        print(file, len(os.listdir(dirname + file)))\n",
    "        idx = 0\n",
    "        try:\n",
    "            for f in os.listdir(dirname + file):\n",
    "                if '.DS_Store' in f:\n",
    "                    continue\n",
    "\n",
    "                files = os.listdir(dirname + file + '/' + f)\n",
    "                length = len(files)\n",
    "                print(f, length)\n",
    "\n",
    "                if length >= 60:\n",
    "                    idx += 1            \n",
    "                else:\n",
    "                    missing = [hyperparams[idx] for idx, item in enumerate(filenames) if item not in files]\n",
    "                    TODO.extend(missing)\n",
    "\n",
    "            print('{0}/20'.format(idx))\n",
    "            print('\\n')\n",
    "        except NotADirectoryError:\n",
    "            files = os.listdir(dirname + file)\n",
    "            missing = [hyperparams[idx] for idx, item in enumerate(filenames) if item not in files]\n",
    "            TODO.extend(missing)\n",
    "            \n",
    "        \n",
    "    return TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/Users/sob/Desktop/apple/64_dims_100000_samples/'\n",
    "outdir = dirname\n",
    "for idx, file in enumerate(os.listdir(dirname)):\n",
    "\n",
    "    if '.DS_Store' in file:\n",
    "        continue\n",
    "\n",
    "    for nest in crawl_directory(dirname + file):\n",
    "\n",
    "        index = 1\n",
    "\n",
    "        if 'dims' not in nest.split('/')[6]:\n",
    "            outdir = '/'.join(nest.split('/')[:6]) + '/'# + nest.split('/')[8:9]) + '/'\n",
    "        else:\n",
    "            # Uncomment the + for mixture\n",
    "            outdir = dirname + nest.split('/')[6]\n",
    "        \n",
    "        print(outdir)\n",
    "            \n",
    "#        # Initialize directory\n",
    "        if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "            os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "        try:\n",
    "            shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "        except:\n",
    "            extension = nest.split('/')[-1]\n",
    "            while os.path.exists(outdir + 'trial_{0}/'.format(index) + extension):\n",
    "                index += 1\n",
    "\n",
    "            if not os.path.exists(outdir + 'trial_{0}/'.format(index)):\n",
    "                os.makedirs(outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "            shutil.move(nest, outdir + 'trial_{0}/'.format(index))\n",
    "\n",
    "remove_empty_dirs(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MOVE HYPERTUNING RESULTS TO BEST FOLDER \"\"\"\n",
    "import tqdm\n",
    "best_path = '/Users/sob/Desktop/gan_results/best/multivariate/64_dims_100000_samples/'\n",
    "dirname = '/Users/sob/Desktop/gan_results/hypertuning/multivariate/64_dims_100000_samples/'\n",
    "if not os.path.exists(best_path):\n",
    "    os.makedirs(best_path)\n",
    "\n",
    "files = os.listdir(dirname)\n",
    "files = [f for f in files if f != '.DS_Store']\n",
    "for idx, f in tqdm.tqdm_notebook(enumerate(files)):\n",
    "    \n",
    "    optimal = get_best_performance_mnist(dirname + f + '/')\n",
    "    if len(os.listdir(dirname + f + '/')) < 60:\n",
    "        print(f, len(os.listdir(dirname + f + '/')))\n",
    "    \n",
    "    with open(best_path + '/trial_{0}.json'.format(idx+1), 'w') as outfile:\n",
    "        json.dump(optimal, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
